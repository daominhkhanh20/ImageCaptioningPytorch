{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FYGs4KMjb8b",
    "outputId": "44eb6389-12ae-4a76-d31f-0651ac008003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pgMfYEVjemY",
    "outputId": "b0b9ad18-8086-44c8-e284-d8810a0ba731",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME']=\"daominhkhanh\"\n",
    "os.environ['KAGGLE_KEY']=\"90e6f266906e9dcfeae1afea9a0630ac\"\n",
    "!kaggle datasets download -d adityajn105/flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZvbrHYnjeo7"
   },
   "outputs": [],
   "source": [
    "!unzip flickr8k.zip\n",
    "!mkdir Flickr8k\n",
    "!mv Images Flickr8k\n",
    "!rm caption.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAclUmBCzi1f"
   },
   "outputs": [],
   "source": [
    "!rm flickr8k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWMkH91wxPb0",
    "outputId": "61a06b30-e4c8-4054-b9c6-6f67a1ceaaa2"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d authman/pickled-glove840b300d-for-10sec-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnkXUVkcyhoi",
    "outputId": "6dbc020c-965f-4a38-b4da-9fdf9baa598d"
   },
   "outputs": [],
   "source": [
    "!unzip pickled-glove840b300d-for-10sec-loading.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6nRR3SlPjesC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split,Dataset\n",
    "from torchvision import transforms as T\n",
    "import pickle\n",
    "from torch import nn\n",
    "import numpy as np \n",
    "from PIL import Image \n",
    "import pandas as pd \n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from torchvision import transforms as T\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXHENLbZ8dGy"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive//MyDrive/Attention2/captions.txt /content/Flickr8k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBv00xyq6VER"
   },
   "outputs": [],
   "source": [
    "embedding_dict=np.load('glove.840B.300d.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WMEzAoaX3Gup"
   },
   "outputs": [],
   "source": [
    "contractions_dict = { \"ain’t\": \"are not\", \"’s\":\" is\", \"aren’t\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"‘cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\", \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he would\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"I’d\": \"I would\", \"I’d’ve\": \"I would have\", \"I’ll\": \"I will\", \"I’ll’ve\": \"I will have\", \"I’m\": \"I am\", \"I’ve\": \"I have\", \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"let’s\": \"let us\", \"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\", \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\", \"she’ll’ve\": \"she will have\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\",\"they’ll\": \"they will\",\n",
    " \"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\",\"what’ll\": \"what will\", \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\", \"what’ve\": \"what have\", \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’ve\": \"where have\",\n",
    " \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\", \"who’ve\": \"who have\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\", \"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\", \"you’re\": \"you are\", \"you’ve\": \"you have\"}\n",
    "contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22mxCbUSjeuw"
   },
   "outputs": [],
   "source": [
    "captions_length=[]\n",
    "data=pd.read_csv('/content/Flickr8k/captions.txt',sep=',',skipinitialspace=True)\n",
    "data.caption=data.caption.apply(lambda x:expand_contractions(x,contractions_dict))\n",
    "table=str.maketrans('','',punctuation)\n",
    "data.caption=data.caption.apply(lambda x:x.translate(table))\n",
    "data.caption=data.caption.apply(lambda x:re.sub(\"\\s\\s+\",\" \",x.lower()))\n",
    "for caption in data.caption:\n",
    "    captions_length.append(len(caption.split(' ')))\n",
    "\n",
    "data['length']=captions_length\n",
    "n_images=len(set(data.image))\n",
    "images=set(data.image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGfDM-JJjexw",
    "outputId": "30ab07c6-b85f-409d-be98-02de3ca72ad2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"We have {n_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnhYjub_je0r"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "images_train=set(random.sample(images,k=6500))\n",
    "images_test=images-images_train\n",
    "images_train=list(images_train)\n",
    "images_test=list(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5fq4kTBje54"
   },
   "outputs": [],
   "source": [
    "data_train=data[data.image.isin(images_train)]\n",
    "data_test=data[data.image.isin(images_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1RUZZmcje80"
   },
   "outputs": [],
   "source": [
    "def preprocessing(dataframe):\n",
    "    images_deleted=set(dataframe[dataframe.length<6].image)\n",
    "    dataframe=dataframe[~dataframe.image.isin(images_deleted)]\n",
    "    dataframe=dataframe.sample(frac=1)\n",
    "    dataframe.drop('length',axis=1,inplace=True)\n",
    "    return dataframe\n",
    "    \n",
    "data_train=preprocessing(data_train)\n",
    "data_test=preprocessing(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6e1tKMajfCy",
    "outputId": "27b7a442-a963-40f5-9347-5227ca2c19cb"
   },
   "outputs": [],
   "source": [
    "print(f\"We have {len(set(data_train.image))} images train\")\n",
    "print(f\"We have {len(set(data_test.image))} images test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBHHPV31jfGF"
   },
   "outputs": [],
   "source": [
    "data_train.to_csv('Flickr8k/captions_train.csv',sep='|',index=False)\n",
    "data_test.to_csv('Flickr8k/captions_test.csv',sep='|',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZ9EdoOYM50K"
   },
   "outputs": [],
   "source": [
    "!cp /content/Flickr8k/captions_train.csv /content/drive/MyDrive/Attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47-JMQWuueOR"
   },
   "outputs": [],
   "source": [
    "!cp /content/Flickr8k/captions_test.csv /content/drive/MyDrive/Attention2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy6Ov6l9AtiN"
   },
   "outputs": [],
   "source": [
    "vocab_size=6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0OlRkjwjfJR"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.vocab_size=6000\n",
    "        self.tokenizer=Tokenizer(num_words=self.vocab_size,\n",
    "                                filters='!\"#$%&()*+/:;\"=?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                oov_token='<unk>'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_size)\n",
    "\n",
    "    def build_vocab(self,captions_list):\n",
    "        self.tokenizer.fit_on_texts(captions_list)\n",
    "        self.sequences=self.tokenizer.texts_to_sequences(captions_list)\n",
    "        self.tokenizer.word_index['<pad>']=0\n",
    "        self.tokenizer.index_word[0]='<pad>'   \n",
    "        self.stoi=self.tokenizer.word_index\n",
    "        self.itos=self.tokenizer.index_word\n",
    "\n",
    "    def numericalize(self,text):\n",
    "        return self.tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "\n",
    "class Flickr8k(Dataset):\n",
    "    def __init__(self,root_dir,caption_file,transforms=None):\n",
    "        path=os.getcwd()\n",
    "        self.transforms=transforms\n",
    "        path_to_caption_file=os.path.join(path,root_dir+\"/\"+caption_file)\n",
    "        df=pd.read_csv(path_to_caption_file,sep='|',skipinitialspace=True)\n",
    "        self.images=df.image\n",
    "        self.captions='<sos> '+df.caption+' <eos>'\n",
    "        self.captions=self.captions.tolist()\n",
    "        self.vocabulary=Vocabulary()\n",
    "        self.vocabulary.build_vocab(self.captions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        image_name=self.images[idx]\n",
    "        path=os.path.join(os.getcwd(),\"Flickr8k/Images/\"+str(image_name))\n",
    "        image=Image.open(path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            image=self.transforms(image)\n",
    "        caption_vector=self.vocabulary.sequences[idx]\n",
    "        return image,torch.tensor(caption_vector)\n",
    "\n",
    "\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self,pad_idx):\n",
    "        self.pad_idx=pad_idx\n",
    "\n",
    "    def __call__(self,batch):\n",
    "        #batch has a lot of pair (image,caption)      \n",
    "        images=[item[0].unsqueeze(dim=0) for item in batch]\n",
    "        captions=[item[1] for item in batch]\n",
    "        images=torch.cat(images,dim=0)#batch_size*3*height*width\n",
    "        captions=pad_sequence(captions,batch_first=True,padding_value=self.pad_idx)\n",
    "        return images,captions\n",
    "\n",
    "\n",
    "def get_dataset(root_dir,caption_file,transforms,batch_size=64,num_workers=2,shuffle=False,pin_memory=True):\n",
    "    dataset=Flickr8k(root_dir=root_dir,\n",
    "                    caption_file=caption_file,\n",
    "                    transforms=transforms,\n",
    "    )\n",
    "    pad_idx=dataset.vocabulary.stoi['<pad>']\n",
    "\n",
    "    data_loader=DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=shuffle,\n",
    "                            pin_memory=pin_memory,#if pin_memory=True, which enables fast data transfer to CUDA-enable GPUS\n",
    "                            collate_fn=MyCollate(pad_idx)\n",
    "    )\n",
    "    return dataset,data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1FRhTM_jfMl"
   },
   "outputs": [],
   "source": [
    "transforms=T.Compose([\n",
    "        T.Resize(226),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485,0.456,0.406),std=(0.229,0.224,0.225))\n",
    "    ])\n",
    "dataset,data_loader=get_dataset(\n",
    "            root_dir=\"Flickr8k\",\n",
    "            caption_file=\"captions_train.csv\",\n",
    "            transforms=transforms\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8_QaT9Nj2vr"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_idx,val_idx=train_test_split(np.arange(len(data_loader)),test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlcY1ju8juZH"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/Attention2/dataset.pickle','wb') as file:\n",
    "    pickle.dump(dataset,file,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lotBHiE5t4YU"
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv('Flickr8k/captions_test.csv',sep='|',skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlMZbg1TbqCJ",
    "outputId": "d69c2edb-3221-438e-bf11-0a705bb26202"
   },
   "outputs": [],
   "source": [
    "print(len(dataset.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLT0yKfSjufW"
   },
   "outputs": [],
   "source": [
    "beam_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cwuMoQIjuh8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "import numpy as np \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.nn import functional as F\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,units=512,encode_image_size=8):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.resnet=models.resnet101(pretrained=True)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "        modules=list(self.resnet.children())[:-2]\n",
    "        self.resnet=nn.Sequential(*modules)\n",
    "        self.adap=nn.AdaptiveAvgPool2d((encode_image_size,encode_image_size))\n",
    "\n",
    "    def forward(self,images):\n",
    "        features=self.resnet(images)\n",
    "        features=self.adap(features)\n",
    "        features=features.permute(0,2,3,1)\n",
    "        features=features.view(features.size(0),-1,features.size(-1))#batch_size*num_pixels*encoder_dim\n",
    "        return features\n",
    "\n",
    "class AttentionBahdanau(nn.Module):\n",
    "    def __init__(self,attention_dim,encoder_dim,decoder_dim):\n",
    "        super(AttentionBahdanau,self).__init__()\n",
    "        self.attention_dim=attention_dim\n",
    "        self.encoder_dim=encoder_dim\n",
    "        self.decoder_dim=decoder_dim\n",
    "\n",
    "        self.weight_decoder=nn.Linear(decoder_dim,attention_dim)\n",
    "        self.weight_encoder=nn.Linear(encoder_dim,attention_dim)\n",
    "        self.full=nn.Linear(attention_dim,1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,encoder_out,hidden_decoder):\n",
    "        #encoder_out: batch_size*num_pixel*encoder_dim\n",
    "        #hidden_decoder: batch_size*decoder_dim\n",
    "\n",
    "        energies=self.full(\n",
    "                torch.tanh(\n",
    "                    self.weight_encoder(encoder_out)+self.weight_decoder(hidden_decoder).unsqueeze(1)\n",
    "                    )\n",
    "            )#batch_size*num_pixels*1\n",
    "        energies=energies.squeeze(dim=2)\n",
    "        attention_weights=self.softmax(energies)#batch_size*num_pixels\n",
    "        context_vector=(encoder_out*attention_weights.unsqueeze(dim=2)).sum(dim=1)#batch_size*encoder_dim\n",
    "        return context_vector,attention_weights\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,attention_dim,decoder_dim,embedding_dim,vocab_size,vocabulary,encoder_dim=2048,drop_pro=0.2):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.attention_dim=attention_dim\n",
    "        self.encoder_dim=encoder_dim\n",
    "        self.decoder_dim=decoder_dim\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.vocab_size=vocab_size\n",
    "        self.vocabulary=vocabulary\n",
    "        self.beam_size=3\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.attention=AttentionBahdanau(attention_dim,encoder_dim,decoder_dim)\n",
    "        self.lstm_cell1=nn.LSTMCell(input_size=embedding_dim+encoder_dim,\n",
    "                        hidden_size=decoder_dim,\n",
    "                        bias=True\n",
    "        )   \n",
    "\n",
    "        self.fcn=nn.Linear(decoder_dim,vocab_size)\n",
    "        self.dropout=nn.Dropout(drop_pro)\n",
    "        self.init_h=nn.Linear(encoder_dim,decoder_dim)\n",
    "        self.init_c=nn.Linear(encoder_dim,decoder_dim)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.init_weights(pretrain_embedding=True)\n",
    "  \n",
    "    \n",
    "    def fine_tune_embeddings(self,fine_tune=True):\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad=fine_tune\n",
    "    \n",
    "    def load_pretrained_embedding(self):        \n",
    "        embedding_matrix=np.zeros((self.vocab_size+1,self.embedding_dim))\n",
    "        for word,idx in self.vocabulary.stoi.items():\n",
    "            if idx >vocab_size:\n",
    "              break\n",
    "            embedding_vector=embedding_dict.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[idx]=embedding_vector\n",
    "            elif word in ['<pad>','<sos>','<eos>']:\n",
    "                embedding_matrix[idx]=np.random.rand(1,300)\n",
    "            else:\n",
    "              embedding_matrix[idx]=embedding_dict['<unk>']\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def init_weights(self,pretrain_embedding=False):\n",
    "        if pretrain_embedding is False:\n",
    "          self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "          self.load_embedding_pretrained()\n",
    "          self.fine_tune_embeddings(fine_tune=False)\n",
    "        self.fcn.bias.data.fill_(0)\n",
    "        self.fcn.weight.data.uniform_(-0.1,0.1)\n",
    "\n",
    "    def load_embedding_pretrained(self):\n",
    "        embeddings=self.load_pretrained_embedding()\n",
    "        embeddings=torch.from_numpy(embeddings.astype(np.float32)).to(device)\n",
    "        self.embedding.weight=nn.Parameter(embeddings)\n",
    "    \n",
    "    def fine_tune_embeddings(self,fine_tune=True):\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad=fine_tune\n",
    "\n",
    "    def init_hidden_state(self,features):\n",
    "        mean_features=features.mean(dim=1)\n",
    "        h1=self.init_h(mean_features)#batch_size*decoder_dim\n",
    "        c1=self.init_c(mean_features)\n",
    "        return h1,c1\n",
    "\n",
    "    def forward(self,features,captions):\n",
    "        #features: batch_size*num_pixels*encoder_dim\n",
    "        #captions: batch_size*seq_length\n",
    "        embedded=self.embedding(captions)#batch_size*seq_length*embedding_size\n",
    "        seq_length=captions.size(1)-1\n",
    "        batch_size=captions.size(0)\n",
    "        h1,c1=self.init_hidden_state(features)\n",
    "        num_pixels=features.size(1)\n",
    "\n",
    "        preds=torch.zeros(batch_size,seq_length,self.vocab_size).to(device)\n",
    "        attention_weights=torch.zeros(batch_size,seq_length,num_pixels).to(device)\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            #context_vectore:batch_size*encoder_dim\n",
    "            #attention_weight:batch_size*num_pixel\n",
    "            context_vector,attention_weight=self.attention(features,h1)\n",
    "            attention_weights[:,i,:]=attention_weight\n",
    "            lstm_input=torch.cat((embedded[:,i,:],context_vector),dim=1)\n",
    "            h1,c1=self.lstm_cell1(lstm_input,(h1,c1))#batch_size*decoder_dim\n",
    "            outputs=self.fcn(self.dropout(h1))#batch_size*vocab_size\n",
    "            preds[:,i,:]=outputs\n",
    "        return preds,attention_weights\n",
    "\n",
    "\n",
    "    def image_captions(self,feature,vocabulary,max_length=30):\n",
    "        start_word=torch.tensor(vocabulary.stoi['<sos>']).view(1,-1).to(device)\n",
    "        embedding=self.embedding(start_word)#1*batch_size*embedding_size\n",
    "        h1,c1=self.init_hidden_state(feature)\n",
    "        attention_weights=[]\n",
    "        caption=[]\n",
    "        for _ in range(max_length):\n",
    "            context,attention_weight=self.attention(feature,h1)\n",
    "            attention_weights.append(attention_weight)\n",
    "            lstm_in=torch.cat((embedding[:,0,:],context),axis=1)\n",
    "            h1,c1=self.lstm_cell1(lstm_in,(h1,c1))#batch_size*decoder_dim\n",
    "            output=self.fcn(h1)\n",
    "            predict_word=output.argmax(dim=1)\n",
    "            if vocabulary.itos[predict_word.item()]==\"<eos>\":\n",
    "                break\n",
    "            \n",
    "            caption.append(predict_word.item())\n",
    "            embedding=self.embedding(predict_word.unsqueeze(dim=0))\n",
    "\n",
    "        return [vocabulary.itos[index] for index in caption]\n",
    "\n",
    "\n",
    "\n",
    "    def through_model(self,feature,word,hidden_state,beam_size):\n",
    "        h1,c1=hidden_state\n",
    "        word=torch.tensor(word).view(1,-1).to(device)\n",
    "        embedding=self.embedding(word)\n",
    "        context_vector,_=self.attention(feature,h1)\n",
    "        lstm_in=torch.cat((embedding[:,0,:],context_vector),dim=1)\n",
    "        h1,c1=self.lstm_cell1(lstm_in,(h1,c1))\n",
    "        #output self.softmax() 1*vocab_size\n",
    "        output=self.softmax(self.fcn(h1)).cpu().detach().numpy()[0]\n",
    "        preds=np.argsort(output)[-beam_size:]\n",
    "        return preds,[h1,c1],output\n",
    "\n",
    "    def image_caption_with_beam_search(self,feature,vocabulary,beam_size,max_length=30,max_collection=15):\n",
    "        hidden_state=self.init_hidden_state(feature)\n",
    "        preds,hidden_state,output=self.through_model(feature,vocabulary.stoi['<sos>'],hidden_state,beam_size)\n",
    "        words=[]\n",
    "        cap_collection=[]\n",
    "        prob_likelihood=list()\n",
    "        has_full=False\n",
    "        for i in range(beam_size):\n",
    "            h1,c1=hidden_state\n",
    "            words.append([[h1,c1],[[preds[i]],output[i]]])\n",
    "        \n",
    "        for _ in range(max_length-1):\n",
    "            temp=[]\n",
    "            for idx,[hidden_state_curr,pair] in enumerate(words):\n",
    "                last_word=pair[0][-1]\n",
    "                preds,hidden_state_curr,output=self.through_model(feature,last_word,hidden_state_curr,beam_size)\n",
    "\n",
    "                for index in preds:\n",
    "                    cap_current,prob=pair[0].copy(),pair[1]\n",
    "                    prob+=np.log(output[index])\n",
    "                    if vocabulary.itos[index]==\"<eos>\":\n",
    "                        cap_collection.append(cap_current)\n",
    "                        prob_likelihood.append(prob/(len(cap_current)+1))\n",
    "                        if len(cap_collection)==max_collection:\n",
    "                            has_full=True\n",
    "                            break\n",
    "                    \n",
    "                    else:\n",
    "                        cap_current.append(index)\n",
    "                        temp.append([hidden_state_curr,[cap_current,prob]])\n",
    "                    \n",
    "            \n",
    "            if has_full is True or len(temp)==0:\n",
    "                break\n",
    "\n",
    "            words=temp\n",
    "            words=sorted(words,key=lambda l:l[1][1],reverse=False)[-beam_size:]\n",
    "\n",
    "        if len(cap_collection)==0:\n",
    "          return []\n",
    "        index=prob_likelihood.index(max(prob_likelihood))\n",
    "        caption=cap_collection[index]\n",
    "        temp=[vocabulary.itos[index] for index in caption]\n",
    "        return temp      \n",
    "    \n",
    "    def beam_search(self,feature,vocabulary,max_length=40,beam_size=3,max_collection=10):\n",
    "        k=beam_size\n",
    "        #encoder_image_size=8\n",
    "        encoder_dim=feature.size(-1)\n",
    "        num_pixel=feature.size(1)\n",
    "\n",
    "        feature=feature.expand(k,num_pixel,encoder_dim)\n",
    "        #tensor store k previous word at each step, now they're just <sos>\n",
    "        k_prev_words=torch.LongTensor([[vocabulary.stoi['<sos>']]] * k).to(device)#(k,1)\n",
    "        \n",
    "        #tensor store top k sequences\n",
    "        seqs=k_prev_words\n",
    "        top_k_scores=torch.zeros(k,1).to(device)\n",
    "        complete_seqs=list()\n",
    "        complete_seq_scores=list()\n",
    "\n",
    "        step=1\n",
    "        h1,c1=self.init_hidden_state(feature)\n",
    "        #s is a number less than or equal to k, because sequences are removed from this \n",
    "        #process once they hit <end>\n",
    "        size_collection=0\n",
    "        while True:\n",
    "            embeddings=self.embedding(k_prev_words).squeeze(1)#s*embedding_dim\n",
    "            contex_vec,_=self.attention(feature,h1)\n",
    "            lstm_in=torch.cat((embeddings,contex_vec),dim=1)\n",
    "            h1,c1=self.lstm_cell1(lstm_in,(h1,c1))\n",
    "            output=self.fcn(h1)\n",
    "            scores=F.log_softmax(output,dim=1)\n",
    "            scores=top_k_scores.expand_as(scores)+scores\n",
    "\n",
    "            if step==1:\n",
    "                top_k_scores,top_k_words=scores[0].topk(k,0,True,True)\n",
    "            else:\n",
    "                top_k_scores,top_k_words=scores.view(-1).topk(k,0,True,True)\n",
    "\n",
    "            pre_word_indexs=top_k_words / vocab_size\n",
    "            pre_word_indexs=pre_word_indexs.type(torch.long)\n",
    "            next_word_indexs=top_k_words%vocab_size\n",
    "            seqs=torch.cat((seqs[pre_word_indexs],next_word_indexs.unsqueeze(1)),dim=1)\n",
    "            incomplete_indexs=[id for id, next_word in enumerate(next_word_indexs) if next_word != vocabulary.stoi[\"<eos>\"]]\n",
    "            complete_indexs=list(set(range(len(next_word_indexs)))-set(incomplete_indexs))\n",
    "\n",
    "            if len(complete_indexs)>0:\n",
    "                complete_seqs.extend(seqs[complete_indexs].tolist())\n",
    "                complete_seq_scores.extend(top_k_scores[complete_indexs])\n",
    "                size_collection+=len(complete_indexs)\n",
    "                \n",
    "            \n",
    "            #k-=len(complete_indexs)\n",
    "            if size_collection>= max_collection:\n",
    "              break\n",
    "            \n",
    "            seqs=seqs[incomplete_indexs]\n",
    "            h1 = h1[pre_word_indexs[incomplete_indexs]]\n",
    "            c1 = c1[pre_word_indexs[incomplete_indexs]]\n",
    "\n",
    "            feature=feature[pre_word_indexs[incomplete_indexs]]\n",
    "            top_k_scores=top_k_scores[incomplete_indexs].unsqueeze(1)\n",
    "            k_prev_words=next_word_indexs[incomplete_indexs].unsqueeze(1)\n",
    "\n",
    "            if step>max_length:\n",
    "                break\n",
    "            step+=1\n",
    "        if len(complete_seq_scores)==0:\n",
    "          return []\n",
    "        index=complete_seq_scores.index(max(complete_seq_scores))\n",
    "        temp=complete_seqs[index]\n",
    "        temp=temp[1:-1]\n",
    "        return [vocabulary.itos[index] for index in temp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "20ccf7895ef14ec296b88d49507ca8be",
      "6f90494388e24071a71cb451036eb74c",
      "296e58c4c4c54f62b55657cf45e74a87",
      "cee274b32e6240f4a1ffafe595bdc15a",
      "1c51e7af3cf44266abc2133637aaeb7f",
      "0ea557dfeb464e7a860d74a7dae4055d",
      "cceb87d88a3a4d7e872dae0e1ee4bd8d",
      "6fed9ff753fb4389b783c58fb2c5ecd5"
     ]
    },
    "id": "cM7lst01j0E-",
    "outputId": "746fba4b-a0b3-42cc-96a5-1e1789a8fcfc"
   },
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=300\n",
    "attention_dim=512\n",
    "decoder_dim=512\n",
    "embedding_dim=300\n",
    "encoder=Encoder().to(device)\n",
    "encoder_optim=optim.RMSprop(encoder.parameters(),lr=1e-4)\n",
    "decoder=Decoder(attention_dim=attention_dim,decoder_dim=decoder_dim,embedding_dim=embedding_dim,vocab_size=vocab_size,vocabulary=dataset.vocabulary).to(device)\n",
    "decoder_optim=optim.RMSprop(decoder.parameters(),lr=4e-4)\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zf9VfSOyjucU"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from matplotlib import pyplot as plt \n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "#from bleu import sentence_bleu,SmoothingFunction\n",
    "\n",
    "smooth=SmoothingFunction()\n",
    "\n",
    "def save_loss(loss_train,epoch):\n",
    "    train=np.asarray(loss_train)\n",
    "    np.save('/content/drive/MyDrive/Attention2/train{}.npy'.format(epoch),train)\n",
    "    print(\"save loss done\")\n",
    "\n",
    "\n",
    "def preprocessing(image_name):\n",
    "    image = Image.open(\"Flickr8k/Images/\"+image_name)\n",
    "    image=transforms(image)\n",
    "    return image\n",
    "\n",
    "def get_list_caption(image_name):\n",
    "    indxs=np.where(test.image==image_name)[0]\n",
    "    result=[]\n",
    "    for index in indxs:\n",
    "        s=str(test.iloc[index]['caption'])\n",
    "        s=s.strip()\n",
    "        result.append(s.split(' '))\n",
    "\n",
    "    return result\n",
    "\n",
    "def show_image(image,caption,caption_beam,epoch):\n",
    "    image[0]=image[0]*0.229+0.485\n",
    "    image[1]=image[1]*0.224+0.456\n",
    "    image[2]=image[2]*0.225+0.406\n",
    "    image=image.cpu().numpy().transpose((1,2,0))\n",
    "    fig,ax=plt.subplots(1,1)\n",
    "    img=ax.imshow(image,interpolation='nearest')\n",
    "    if caption is not None:\n",
    "        title='Greedy:  '+caption+'\\nBeam size={}: '.format(beam_size)+caption_beam\n",
    "        ax.set_title(title)\n",
    "    plt.savefig('/content/drive/MyDrive/Attention2/image{}.png'.format(epoch))\n",
    "    print(\"save image done\")\n",
    "\n",
    "\n",
    "def evaluate_bleu_score_test_set(epoch):\n",
    "    bleu1,bleu2,bleu3,bleu4=0,0,0,0\n",
    "    bleu1_beam,bleu2_beam,bleu3_beam,bleu4_beam=0,0,0,0\n",
    "    images=set(test.image)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    file=open('/content/drive/MyDrive/Attention2/caption_result{}.txt'.format(epoch),'w')\n",
    "    file.write(f'image,captionsn,caption_beam_size_{beam_size}\\n')\n",
    "    t=randint(0,len(images))\n",
    "    with torch.no_grad():\n",
    "        for idx,image_name in enumerate(images):\n",
    "            image=preprocessing(image_name)\n",
    "            image=image.to(device)\n",
    "            feature=encoder(image.unsqueeze(dim=0))\n",
    "            caption_greedy=decoder.image_captions(feature,dataset.vocabulary)\n",
    "            caption_beam_search=decoder.beam_search(feature,dataset.vocabulary,beam_size=beam_size)\n",
    "            if len(caption_beam_search)==0:\n",
    "              continue\n",
    "            list_caption=get_list_caption(image_name)\n",
    "            caption=' '.join([word for word in caption_greedy])\n",
    "            cap_beam=' '.join([word for word in caption_beam_search])\n",
    "            \n",
    "            file.write(f'{image_name},{caption},{cap_beam}\\n')\n",
    "            bleu1+=sentence_bleu(list_caption,caption_greedy,weights=(1,0,0,0))\n",
    "            bleu2+=sentence_bleu(list_caption,caption_greedy,weights=(0.5,0.5,0,0))\n",
    "            bleu3+=sentence_bleu(list_caption,caption_greedy,weights=(0.333,0.333,0.333,0))\n",
    "            t4_1=sentence_bleu(list_caption,caption_greedy,weights=(0.25,0.25,0.25,0.25))\n",
    "            bleu4+=t4_1\n",
    "\n",
    "            bleu1_beam+=sentence_bleu(list_caption,caption_beam_search,weights=(1,0,0,0))\n",
    "            bleu2_beam+=sentence_bleu(list_caption,caption_beam_search,weights=(0.5,0.5,0,0))\n",
    "            bleu3_beam+=sentence_bleu(list_caption,caption_beam_search,weights=(0.333,0.333,0.333,0))\n",
    "            t4_2=sentence_bleu(list_caption,caption_beam_search,weights=(0.25,0.25,0.25,0.25))\n",
    "            bleu4_beam+=t4_2\n",
    "            if idx==t:\n",
    "              print(image_name)\n",
    "              print(caption)\n",
    "              print(cap_beam)\n",
    "              show_image(image,caption,cap_beam,epoch)\n",
    "    file.close()\n",
    "    print(\"Greedy seach:\")\n",
    "    print(\"Bleu for 1-gram:\",bleu1*1.0/(len(images)-cnt))\n",
    "    print(\"Bleu for 2-gram:\",bleu2*1.0/(len(images)-cnt))\n",
    "    print(\"Bleu for 3-gram:\",bleu3*1.0/(len(images)-cnt))\n",
    "    print(\"Bleu for 4-gram:\",bleu4*1.0/(len(images)-cnt))\n",
    "\n",
    "    print(\"Beam seach:\")\n",
    "    print(\"Bleu for 1-gram:\",bleu1_beam*1.0/(len(images)-cnt))\n",
    "    print(\"Bleu for 2-gram:\",bleu2_beam*1.0/(len(images)-cnt))\n",
    "    print(\"Bleu for 3-gram:\",bleu3_beam*1.0/(len(images)-cnt))\n",
    "    print(\"Bleu for 4-gram:\",bleu4_beam*1.0/(len(images)-cnt))\n",
    "def save_model(epoch):\n",
    "    model_state={\n",
    "        'epochs':epochs,\n",
    "        'encoder_state_dict':encoder.state_dict(),\n",
    "        'decoder_state_dict':decoder.state_dict(),\n",
    "        'encoder_optim_state_dict':encoder_optim.state_dict(),\n",
    "        'decoder_optim_state_dict':encoder_optim.state_dict()\n",
    "    }\n",
    "    torch.save(model_state,\"/content/drive/MyDrive/Attention2/model{}.pth\".format(epoch))\n",
    "    print(\"save model done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63SqzVbaj0IE",
    "outputId": "55673101-d6d2-4b70-99cf-bf8a85cc64de"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "loss_train=[]\n",
    "for epoch in range(0,epochs+1):\n",
    "    start_time=time.time()\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for idx,(images,captions) in enumerate(data_loader):\n",
    "        images=images.to(device)\n",
    "        captions=captions.to(device)\n",
    "        features=encoder(images)\n",
    "        outputs,attention_weights=decoder(features,captions)\n",
    "        targets=captions[:,1:]\n",
    "        loss=criterion(outputs.reshape(-1,vocab_size),targets.reshape(-1))\n",
    "        encoder_optim.zero_grad()\n",
    "        decoder_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optim.step()\n",
    "        decoder_optim.step()\n",
    "        if(idx%50==0):\n",
    "          print(idx,end=' ')\n",
    "    print()\n",
    "\n",
    "    loss_train.append(loss.item())\n",
    "    print(\"Epoch:{}----Train:{}----Run:{}s\".format(epoch,loss.item(),-start_time+time.time()))\n",
    "    if epoch!=0 and epoch %5==0:\n",
    "        save_loss(loss_train,epoch)\n",
    "        save_model(epoch)\n",
    "        evaluate_bleu_score_test_set(epoch)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Flickr8k_beam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ea557dfeb464e7a860d74a7dae4055d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c51e7af3cf44266abc2133637aaeb7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "20ccf7895ef14ec296b88d49507ca8be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_296e58c4c4c54f62b55657cf45e74a87",
       "IPY_MODEL_cee274b32e6240f4a1ffafe595bdc15a"
      ],
      "layout": "IPY_MODEL_6f90494388e24071a71cb451036eb74c"
     }
    },
    "296e58c4c4c54f62b55657cf45e74a87": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ea557dfeb464e7a860d74a7dae4055d",
      "max": 178728960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c51e7af3cf44266abc2133637aaeb7f",
      "value": 178728960
     }
    },
    "6f90494388e24071a71cb451036eb74c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fed9ff753fb4389b783c58fb2c5ecd5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cceb87d88a3a4d7e872dae0e1ee4bd8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee274b32e6240f4a1ffafe595bdc15a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fed9ff753fb4389b783c58fb2c5ecd5",
      "placeholder": "​",
      "style": "IPY_MODEL_cceb87d88a3a4d7e872dae0e1ee4bd8d",
      "value": " 170M/170M [00:16&lt;00:00, 11.1MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
